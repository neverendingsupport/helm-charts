<!-- note: this file is generated by helm-doc.
     Edit the values or README.md.gotmpl to make changes -->
# universal-chart

NES Universal Helm Chart

![Version: 1.2.0](https://img.shields.io/badge/Version-1.2.0-informational?style=flat-square) ![Type: application](https://img.shields.io/badge/Type-application-informational?style=flat-square)

## Additional Information

This is supposedly a universal helm chart for "simple" apps.  It has a couple
of helpful extra features:
* built-in support for loading env vars from an AWS Secret Manager secret
* Bitnami redis chart

TODO: Explain how those work better

## Using The Chart

Normally, you're going to want to distribute this chart via ArgoCD as an
Application. Here's an example definition for "eol report card".

```yaml
---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: &appname eol-report-card
  namespace: argocd
spec:
  project: eolds
  sources:
    - repoURL: https://github.com/neverendingsupport/eol-report-card.git
      path: infra/helm-chart/
      targetRevision: main
      helm:
        valueFiles:
          - $app_repo/infra/dev.values.yaml
        valuesObject:
          ingress:
            enabled: true
            className: nginx
            annotations:
              alb.ingress.kubernetes.io/ip-address-type: dualstack
              cert-manager.io/cluster-issuer: letsencrypt-prod
              nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
              forecastle.stakater.com/expose: "true"
              #forecastle.stakater.com/icon: "put a URL to an icon here someday"
            hosts:
              - host: eol-report-card.stage.apps.herodevs.io
                paths:
                  - path: /
                    pathType: Prefix
            tls:
              - secretName: erc-tls
                hosts:
                  - eol-report-card.stage.apps.herodevs.io
    - repoURL: https://github.com/neverendingsupport/eol-report-card.git
      targetRevision: main
      ref: app_repo

  destination:
    server: "https://kubernetes.default.svc"
    namespace: *appname
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
      allowEmpty: false
    syncOptions:
      - Validate=true
      - CreateNamespace=true
      - PrunePropagationPolicy=foreground
      - PruneLast=true
      - RespectIgnoreDifferences=true
    managedNamespaceMetadata:
      labels:
        use-ghcr-pull-secret: "true"

```

Of note: the ingress configuration always goes in the Argo Application, not in
your values YAML. An explanation of how Argo Applications work is documented
elsewhere (TODO: add link here)

## Requirements

| Repository | Name | Version |
|------------|------|---------|
| https://charts.bitnami.com/bitnami | redis | 21.2.14 |

## Values

| Key | Type | Default | Description |
|-----|------|---------|-------------|
| affinity | object | `{}` | Select roughly specific nodes to run upon. This is similar to node selectors, but allows a bit more fuzziness and flexibility. More info at https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity |
| autoscaling | object | `{"enabled":false,"maxReplicas":10,"minReplicas":1,"targetCPUUtilizationPercentage":80,"targetMemoryUtilizationPercentage":null}` | section for configuring autoscaling. More information can be found here: https://kubernetes.io/docs/concepts/workloads/autoscaling/ |
| autoscaling.enabled | bool | `false` | enable autoscaling |
| autoscaling.maxReplicas | int | `10` | maximum number of replicas to run |
| autoscaling.minReplicas | int | `1` | miminum number of replicas to run |
| autoscaling.targetCPUUtilizationPercentage | int | `80` | If CPU utilization of replicas exceeds this percentage of requested CPU, start a new replica |
| autoscaling.targetMemoryUtilizationPercentage | string | `nil` | If Memory utilization of replicas exceeds this percentage of requested Memory, start a new replica |
| awsEnvSecrets.env_secret_name | string | `"aws-env"` | name of secret to store AWS secretmanager values within |
| awsEnvSecrets.externalSecret.secretPath | string | `""` | secret path |
| awsEnvSecrets.externalSecret.secretStoreRef.kind | string | `"SecretStore"` | Is the store in this namespace or cluster-wide? |
| awsEnvSecrets.externalSecret.secretStoreRef.name | string | `"aws-secrets-manager"` | name of the secret store; aws-secret-manager is usually right |
| deployment.annotations | object | `{}` | extra annotations to add to the deployment resource's metadata. These annotations are key-value pairs attached directly to the Deployment resource. They can be used by external tooling, operators, or for tracking deployment metadata and events. For more info, see: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/ |
| extraContainerPorts | list | `[]` | extra ports to be exposed directly from pods (no service) |
| extraContainerProps | object | `{}` | A dictionary of extra attributes to add to the container spec in the deployment. Elements will be directly added to the deployment's `spec.template.spec.containers` object. Note that adding an element already in the deployment template like `env` or `image` will cause undesirable behavior. |
| extraEnvConfigmaps | list | `[]` | extra configmaps to load into environment |
| extraEnvSecrets | list | `[]` | extra secrets to load into environment |
| extraEnvVars | object | `{}` | additional environment variables and their values. Supports both simple string values and complex objects with valueFrom. |
| extraManifests | list | `[]` | A list of extra yaml manifests to include. Each element will be rendered exactly as passed in |
| fullnameOverride | string | `""` |  |
| image.pullPolicy | string | `"Always"` | k8s image pull policy |
| image.repository | string | `nil` | repository path to image without tag name. Example: ghcr.io/neverendingsupport/universal-chart |
| image.tag | string | `nil` | tag / sha of image to pull |
| imagePullSecrets | list | `[]` |  |
| ingress | object | `{"annotations":{},"className":"","enabled":false,"hosts":[],"tls":[]}` | This block is for setting up the ingress. More information can be found here: https://kubernetes.io/docs/concepts/services-networking/ingress/ |
| ingress.annotations | object | `{}` | a map of annotations to define on the ingress resource |
| ingress.className | string | `""` | which ingress class to use (usually nginx, but could be alb) |
| ingress.enabled | bool | `false` | whether or not to use an ingress |
| ingress.hosts | list | `[]` | list of host blocks to listen on; host blocks define more than just a hostname hosts.host -- a hostname to listen upon. If TLS is enabled, the host will be selected via SNI. hosts.paths -- List of path rules.  Normally you want the root for a hostname to go to the root of your app, and the example commented above works well for that. |
| ingress.tls | list | `[]` | list of TLS certs to use.  The objects in the list have a secret name where the cert will be stored and a list of hosts to include in that cert. Normally this will only be a one item list, but it's technically acceptable to create multiple certs. If ingress.tls.secretName isn't specified, the secret will just be named "tls". |
| initContainers | list | `[{"command":[],"extraContainerProps":{},"image":null}]` | define init container(s) which will run before the "real" container starts. The init container runs with the same environment, volumes, and security context as the main container. |
| initContainers[0].command | list | `[]` | the command to run in the init container This overrides the command in the container.  Leave it empty to just run the container's default command. |
| initContainers[0].extraContainerProps | object | `{}` | a map of additional properties for the init container.  This can technically be any values from the spec, though reusing image, command, securityContext, volumes, env, or envFrom might cause unexpected behavior. |
| initContainers[0].image | string | `nil` | the image to run on init if this is left as null or a false-ish value, the initContainers section of the deploment will be skipped |
| livenessProbe | string | `nil` | Configure a liveness probe, please more information can be found here: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/ |
| nameOverride | string | `""` |  |
| nodeSelector | object | `{}` | Select specific nodes to run upon Normally this should be an empty map |
| podAnnotations | object | `{}` | Add additional annotations to the pod. Annotations are generally for "people" uses and interoperability. For more information check out: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/ |
| podLabels | object | `{}` | Add additional labels to the pods. Labels are generally for k8s internal use (pod selectors, etc) For more information check out: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/ |
| podSecurityContext | object | `{}` |  |
| readinessProbe | string | `nil` | Configure a readiness probe, please |
| redis | object | `{"auth":{"enabled":true,"usePasswordFiles":false},"autoscaling":{"enabled":true,"maxReplicas":5,"minReplicas":1,"targetCPU":80,"targetMemory":80},"enabled":false,"image":{"repository":"bitnamilegacy/redis","tag":"8.2.1-debian-12-r0"},"master":{"resourcesPreset":"micro"},"metrics":{"enabled":true,"image":{"repository":"bitnamilegacy/redis-exporter","tag":"1.76.0-debian-12-r0"},"prometheusRule":{"enabled":true,"rules":[{"alert":"RedisDown","annotations":{"description":"Redis(R) instance {{ \"{{ $labels.instance }}\" }} is down","summary":"Redis(R) instance {{ \"{{ $labels.instance }}\" }} down"},"expr":"redis_up{service=\"{{ template \"common.names.fullname\" . }}-metrics\"} == 0","for":"2m","labels":{"severity":"error"}},{"alert":"RedisMemoryHigh","annotations":{"description":"Redis(R) instance {{ \"{{ $labels.instance }}\" }} is using {{ \"{{ $value }}\" }}% of its available memory.\n","summary":"Redis(R) instance {{ \"{{ $labels.instance }}\" }} is using too much memory"},"expr":"redis_memory_used_bytes{service=\"{{ template \"common.names.fullname\" . }}-metrics\"} * 100 / redis_memory_max_bytes{service=\"{{ template \"common.names.fullname\" . }}-metrics\"} > 90\n","for":"2m","labels":{"severity":"error"}},{"alert":"RedisKeyEviction","annotations":{"description":"Redis(R) instance {{ \"{{ $labels.instance }}\" }} has evicted {{ \"{{ $value }}\" }} keys in the last 5 minutes.\n","summary":"Redis(R) instance {{ \"{{ $labels.instance }}\" }} has evicted keys"},"expr":"increase(redis_evicted_keys_total{service=\"{{ template \"common.names.fullname\" . }}-metrics\"}[5m]) > 0\n","for":"1s","labels":{"severity":"error"}}]},"serviceMonitor":{"enabled":true}},"replica":{"resourcesPreset":"micro"},"tls":{"enabled":false}}` | Redis subchart configuration values. Default values are at https://artifacthub.io/packages/helm/bitnami/redis |
| redis.auth | object | `{"enabled":true,"usePasswordFiles":false}` | beware: overriding auth in your values file might be a mistake |
| redis.autoscaling | object | `{"enabled":true,"maxReplicas":5,"minReplicas":1,"targetCPU":80,"targetMemory":80}` | autoscaling is basically always the right answer :D |
| redis.enabled | bool | `false` | whether or not to enable the Bitnami Redis helm chart |
| redis.image | object | `{"repository":"bitnamilegacy/redis","tag":"8.2.1-debian-12-r0"}` | use the bitnamilegacy registry |
| redis.image.tag | string | `"8.2.1-debian-12-r0"` | latest release in bitnamilegacy https://hub.docker.com/r/bitnamilegacy/redis/tags |
| redis.master | object | `{"resourcesPreset":"micro"}` | use presets for resource limits. See https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl |
| redis.metrics | object | `{"enabled":true,"image":{"repository":"bitnamilegacy/redis-exporter","tag":"1.76.0-debian-12-r0"},"prometheusRule":{"enabled":true,"rules":[{"alert":"RedisDown","annotations":{"description":"Redis(R) instance {{ \"{{ $labels.instance }}\" }} is down","summary":"Redis(R) instance {{ \"{{ $labels.instance }}\" }} down"},"expr":"redis_up{service=\"{{ template \"common.names.fullname\" . }}-metrics\"} == 0","for":"2m","labels":{"severity":"error"}},{"alert":"RedisMemoryHigh","annotations":{"description":"Redis(R) instance {{ \"{{ $labels.instance }}\" }} is using {{ \"{{ $value }}\" }}% of its available memory.\n","summary":"Redis(R) instance {{ \"{{ $labels.instance }}\" }} is using too much memory"},"expr":"redis_memory_used_bytes{service=\"{{ template \"common.names.fullname\" . }}-metrics\"} * 100 / redis_memory_max_bytes{service=\"{{ template \"common.names.fullname\" . }}-metrics\"} > 90\n","for":"2m","labels":{"severity":"error"}},{"alert":"RedisKeyEviction","annotations":{"description":"Redis(R) instance {{ \"{{ $labels.instance }}\" }} has evicted {{ \"{{ $value }}\" }} keys in the last 5 minutes.\n","summary":"Redis(R) instance {{ \"{{ $labels.instance }}\" }} has evicted keys"},"expr":"increase(redis_evicted_keys_total{service=\"{{ template \"common.names.fullname\" . }}-metrics\"}[5m]) > 0\n","for":"1s","labels":{"severity":"error"}}]},"serviceMonitor":{"enabled":true}}` | Prometheus Metrics enabled for redis by default |
| redis.metrics.image | object | `{"repository":"bitnamilegacy/redis-exporter","tag":"1.76.0-debian-12-r0"}` | use the bitnamilegacy registry |
| redis.metrics.prometheusRule.rules | list | `[{"alert":"RedisDown","annotations":{"description":"Redis(R) instance {{ \"{{ $labels.instance }}\" }} is down","summary":"Redis(R) instance {{ \"{{ $labels.instance }}\" }} down"},"expr":"redis_up{service=\"{{ template \"common.names.fullname\" . }}-metrics\"} == 0","for":"2m","labels":{"severity":"error"}},{"alert":"RedisMemoryHigh","annotations":{"description":"Redis(R) instance {{ \"{{ $labels.instance }}\" }} is using {{ \"{{ $value }}\" }}% of its available memory.\n","summary":"Redis(R) instance {{ \"{{ $labels.instance }}\" }} is using too much memory"},"expr":"redis_memory_used_bytes{service=\"{{ template \"common.names.fullname\" . }}-metrics\"} * 100 / redis_memory_max_bytes{service=\"{{ template \"common.names.fullname\" . }}-metrics\"} > 90\n","for":"2m","labels":{"severity":"error"}},{"alert":"RedisKeyEviction","annotations":{"description":"Redis(R) instance {{ \"{{ $labels.instance }}\" }} has evicted {{ \"{{ $value }}\" }} keys in the last 5 minutes.\n","summary":"Redis(R) instance {{ \"{{ $labels.instance }}\" }} has evicted keys"},"expr":"increase(redis_evicted_keys_total{service=\"{{ template \"common.names.fullname\" . }}-metrics\"}[5m]) > 0\n","for":"1s","labels":{"severity":"error"}}]` | default rules from the Bitnami chart :shrug: |
| redis.replica | object | `{"resourcesPreset":"micro"}` | use presets for resource limits. See https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl |
| redis.tls.enabled | bool | `false` | enable or disable TLS |
| replicaCount | int | `1` | set a fixed number of replicas in the deployment This value is ignored if autoscaling is enabled |
| resources | object | `{}` | resource requests and limits. typically you can accept the values commented below, but ideally you'd run this in dev with some synthetic load and then either check on the monitoring values from Grafana or look at the Vertical Pod Autoscaler's recomendations via Goldilocks. |
| securityContext | object | `{}` |  |
| service | object | `{"port":3000,"type":"ClusterIP"}` | A "service" is basically a named port which follows a pod or pods; you should always use a service when networking in k8s. More information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/ |
| service.port | int | `3000` | Defines the port the service listens upon. This is the *external* port exposed by the container, not necessarily the internal port inside the container.  It also doesn't have to be 80 or 443; an ingress (if used) will listen on a differnet port and communicate with the container on this service/port combination. more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#field-spec-ports |
| service.type | string | `"ClusterIP"` | Define the service type more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types |
| serviceAccount.annotations | object | `{}` |  |
| serviceAccount.automount | bool | `true` |  |
| serviceAccount.create | bool | `true` |  |
| serviceAccount.name | string | `""` |  |
| serviceMonitor | object | `{"enabled":false,"interval":null,"path":"/metrics"}` | Configure a ServiceMonitor for scraping metrics from the service. |
| serviceMonitor.enabled | bool | `false` | Whether to create a ServiceMonitor resource. |
| serviceMonitor.interval | string | `nil` | Optional scrape interval (in seconds). When null, the operator default is used. |
| serviceMonitor.path | string | `"/metrics"` | HTTP path to scrape for metrics. |
| spread_azs | boolean | `false` | Add a topology spread rule across Karpenter availability zones. |
| spread_spot | boolean | `false` | Add a topology spread rule across Karpenter capacity types (spot vs on-demand). |
| tolerations | list | `[]` | List of taints these pods should tolerate. Normally this should be an empty list |
| topologySpreadConstraints | list | `[{"maxSkew":1,"topologyKey":"topology.kubernetes.io/zone","whenUnsatisfiable":"ScheduleAnyway"}]` | When deploying with multiple replicas, spread pods around using these rules. The default is to spread pods evenly among the Availability Zones defined in the cluster. With a Karpenter-managed EKS cluster (like HeroDevs uses), there will usually be 3 AZs in a region where a cluster is deployed. |
| volumeMounts | list | `[]` | Additional volumes to mount |
| volumes | list | `[]` | Additional volumes to create |
